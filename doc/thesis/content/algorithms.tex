\section{Learning Algorithms}
In this section we apply two algorithms for solving abovementioned poblems \ref{eq:minimisation_setup} and \ref{eq:stabilisation_setup}: the one is PrimalDualBwK from \cite{Badanidiyuru2013}, and another is linCBwK from \cite{Agrawal2015}. We embedded to the later our Markov model, so let us address it as linCBwK-m.

The comparison of these two algorithms is summarised in table \ref{table:algorithms_comparison}

\begin{table}[h!]
    \centering
    \label{table:algorithms_comparison}
    \begin{tabular}{|l|c|c|}
        \hline
         & PrimalDualBwK & linCBwK-m \\
         \hline \hline 
         Is knowledge of k(t) required & Yes & No \\
         \hline 
         Parameters estimation method & UCB and LCB & Confidential Ellipsoids \\
         \hline
         Assumptions on reward structure & No & Linear (\ref{def:linear_relisability_assumption}) \\
         \hline 
         Using Markov model & No & Yes \\
         \hline
    \end{tabular}
    \caption{Comparison of two learning algorithms}
\end{table}

\subsection{PrimalDualBwK}

This algorithm unites UCB approach with Online Stochastic Packing Problem. It uses a simple idea of making decisions according to the highest "bang-per-buck" ratio. It treats set of possible control matrices $\PP$ as an abstract set of $m$ arms and do not use any model assumptions on devices behaviour. However, such simplicity requires to know the number of devices $k(t)$ which accepts Aggregator's requests at the moment $t$. It does not contradict to ensemble approach as $k(t)$ is an aggregated value which does not allow to infer individual users' feedback. Hence it still meets all the requirements. In addition, due to its simplicity, this algorithm will be considered as a baseline approach.

As we do not modify the original algorithm, we do not put the pseudocode here. To obtain it, one may address \cite{Badanidiyuru2013}. 

\subsection{linCBwK-m}

In the following section we treat \ref{eq:minimisation_setup} as a linear contextual bandit with knapsack problem (see \ref{def:lincbwk_setup}). In this form of bandits setup we face two major troubles: 

% Define the policy as some mapping from arm contexts $x_{a,t}$ to the matrix number $i \in \{0, 1, \dots, m\}$ (a.k.a. arm). The goal is to learn the policy $\nu$ which minimises the regret:

%\begin{equation}
%    \begin{split}
%    \min_{\nu}\, & {\cal R}^{\nu}_T = \min_{\nu} \sum_{t=1}^T(r^{\nu}(t) - r^*(t)) \\
%     s.t. & \sum_{t=1}^T k(t) \leq B 
%     \end{split}
%\end{equation}
%where the penalty is defined as
%\[
%    r^{\nu}(t) = c(t)|s^{\nu}(t) - {\bar s}(t)|
%\]
%and $k(t)$ is the number of loads who accepted the curtailment request at the time $t$, $B$ is the contract limit for user disturbance and $\nu$ is the policy being used. Hereafter we refer to $r^{\nu}(t)$ as $r(t)$.


\begin{enumerate}
    \item General bandit with knapsack problem setup  consider penalties as a random independent variables over time and arms. Both assumptions here are false: our choices from the past strongly affects the current reward distributions.
    \item Variables $k(t)$ are unobservable.
\end{enumerate} 

To deal with these issues consider the the mechanics of how $s(t)$ depends on aggregator's actions according to its Markov models. Suppose that by the time $t$ we have a (possibly unknown) state-distribution $\pi(t)$ (see \ref{eq:pi_definition}). Denote the number of devices which have chosen matrix $\bP_j$ at the moment $t$ as $n_j(t)$. Denote also the fraction of devices, which at the moment $t$ accepts matrix $P_i$ having this own behaviour $\bP_j$ as $\Omega_{ij}(t)$. It is easy to see that if we send no control (arm $0$), then the next-moment consumption will be:

    \begin{equation}
    \label{eq:no_control_consumption}
    \begin{split}
        s(t|0) := \sum_{j=1}^{m}n_j(t)q^TP_j\pi(t) =  \sum_{j=1}^{m}n_j(t)[x_{t, 0}]_j = n(t)^Tx_{t, 0}
    \end{split}
    \end{equation}
    
    and if we send $i^{th}$ matrix $P_i$, the consumption will be:

    \begin{equation}
        \label{eq:arm_consumption}
    \begin{split}
        s(t|i) & := \sum_{j=1}^mn_j(t)q^T(\Omega_{ij}(t)P_i + (1-\Omega_{ij}(t))P_j)\pi(t) = \\& = \sum_{j=1}^{m}n_j(t)[x_{t, i}]_j = n(t)^Tx_{t, i}
    \end{split}
    \end{equation}
    
    The model appears to be linear over unknown vector $n^T(t)$ and some vector $x_{t, i}\, i \in [0, 1, \dots m]$ which we know if we have $\pi(t)$ and $\Omega_{ij}(t)$. Therefore it is surprisingly convenient to consider $x_{t, i}$ as a feature vector of an arm $i$ at the moment $t$. Note, that the features $[x_{t, i}]_j$ have the natural interpretation as the amount of energy being consumed by an average device which has $\bP = P_j$ at the time $t$ if we pull the $i$-th arm. Moreover, we also have an estimator for budgeted variable: $k(t|i) = \sum_{i=1}^{m}\Omega_{ij}n_j(t)$.

The above mentioned arm-contexts $x_{i, t}$ reveal a straightforward formulation for the minimisation setup \ref{eq:minimisation_setup}

\begin{align}
    \label{eq:minimization_bandit_setup}
    \begin{split}    
    \min_{\nu:\, t \to \{0, 1\dots m\} }& \sum_{t=1}^Tn(t)^Tx_{t, \nu(t)} = \min_{\nu} \sum_{t=1}^Ts^\nu(t)\\
     s.t. \E& \sum_{t=1}^T k(t) \leq B \\
     & \sum_{j=1}^m n_j(t) = n\, \forall t\\
     & n_j(t) \geq 0\, \forall t
     \end{split}
\end{align} 
 
as well as for stabilisation setup \ref{eq:stabilisation_setup}

\begin{align}    
    \begin{split}
    \label{eq:stabilization_bandit_setup}
    \min_{\nu:\, t \to \{0, 1\dots m\} }& \sum_{t=1}^T|n(t)^Tx_{t, \nu(t)}) - \bs(t)| = \min_{\nu } \sum_{t=1}^T|s^\nu(t) - \bs(t)|\\
     s.t. \E& \sum_{t=1}^T k(t) \leq B \\
     & \sum_{j=1}^m n_j(t) = n\, \forall t \\
     & n_j(t) \geq 0\, \forall t
     \end{split}
\end{align}

By now $n(t)$ is an unknown time series, and to the best of our knowledge there are no general BwK algorithms for such setup. Hence, we assume that $n(t)$ is a periodic function over time with a period of 24 hours (at least within one season of a year), so we can consider \ref{eq:minimization_bandit_setup} and \ref{eq:stabilization_bandit_setup} as a set of 24 bandits: each one is learning to make a decision at the assigned hour. Note that these problems are in exact form for recently emerged budgeted bandit solvers (\cite{Agrawal2015}), and all the requirements are met: the divergence between predicted and modelled consumption may appear only regarding to stochastic fluctuations due to the finiteness of the ensemble, hence we have independence of reward as a function of a context through arms and time. 

The pseudocode of the algorithm is shown in alg \ref{algo:linCBwK-m}. In its design two concepts were used: Multiplicative Online Learning and Confidence Ellipsoid Parameters Estimation:

\paragraph{Confidence Ellipsiod} 
This approach is a common way for deriving upper and lower confidence bounds for unknown vector parameters in linear MABs \cite{Li2011}. The first corollary of the following theorem was proven in \cite{Auer2003}, the second one -- in \cite{Abbasi-Yadkori2011}.  
\begin{theorem}
\label{theorem:ellipsoids}
    Suppose there is some unknown distribution $p(r_t, x_t)$ of pairs $(r_t, x_t)$, where $r_t$ linearly depends on $x_t \in \R^m$ with some zero-mean noise: 
    \[
        r_t = \mu^Tx_t + \nu_t\quad s.t. \, \E[\nu|H_{t-1}] = 0, \, |\nu| \leq R
    \]
    where $\mu \in R^m$ is some unknown parameter which one wants to estimate, $H_{t}$ is the history up to time $t$. 
    
    Define a sequence of matrices $M_t \in \R^{m \times m}$ 
    \[
        M_t := \lambda I + \sum_{\tau = 0}^{t-1}x_\tau x_\tau^T
    \]
    Then, with a probability $\delta$ two following statements hold:
    
    \begin{enumerate}
        \item $\sum_{t=1}^T\|x_t\|_{M_t^{-1}} \leq \sqrt{mT\log{T}}$ where $\|x\|_M := \sqrt{x^TMx}$
        \item $\mu \in C_t$ where 
            \begin{equation*}
            \label{eq:confidential_ellipsoid}
            \begin{split}
                C_t & := \{\mu \in \R^m\, : \, \|\mu - \hat{\mu}\|_{M_t} \leq \frac{R}{2}\sqrt{m\log\left(\frac{1+tm/\lambda}{\delta}\right)} + \sqrt{\lambda m}\} \\
                \hat{\mu_t} & := M^{-1}_t\sum_{\tau = 1}^{t-1}x_ir_i \\
            \end{split}
            \end{equation*}
    \end{enumerate}
\end{theorem} 

\paragraph{Multiplicative Weight Update} This technique is widely used in Online Learning problems due to its simplicity and close-to-optimal regret bounds.

Consider an interaction of an Agent and an Adversary. The Agent chooses some vector $\theta_t \in \Theta =  \{\theta \, | \, \|\theta\|_1 \leq 1, \, \theta \geq 0\}$ and the Adversary picks a linear function $f_t: \, \Theta \to [-1, 1]$. The Agent may use only previous history in his choices. His goal is to minimise the regret defined as: 
    \[
        \RR_{T} := \max_{\theta \in \Theta} \sum_{t = 1}^T g_t(\theta) - g_t(\theta_t)
    \]
    Let the $\theta_t$ be the following:
    
    \begin{equation}
        \label{eq:mwu_theta}
        \begin{split}
            \theta_{t+1, j} := \frac{w_{t, j}}{1 + \sum_{j}w_{t,j}}\quad where  \, w_{t, j} &:= \begin{cases}
                    w_{t-1, j}(1 + \varepsilon)^{g_t(1_j)} & if\, g_t(1_j) > 0 \\
                    w_{t-1, j}(1 - \varepsilon)^{-g_t(1_j)} & if\, g_t(1_j) \leq 0
                    \end{cases}\\
             \, \varepsilon &:= \sqrt{\frac{\log(d+1)}{T}}
        \end{split}
    \end{equation}
    If $\theta$ is in form \ref{eq:mwu_theta}, then the following \todo{cite Arora} theorem holds:
    
\begin{theorem}
    \[
        \RR_T \leq  \sqrt{T\log(d+1)}
    \]
\end{theorem}

\begin{algorithm}
\caption{linCBwK-m}
\label{algo:linCBwK-m}
\begin{algorithmic}[1]
\REQUIRE{${\cal P},\, n,\, \tau,\, B$}
\STATE{Initialise $n_i(0) := 1/n\, \forall i$}
\STATE{Initialise $\pi_0 := \frac{1}{n}\sum_{i=1}^mn_i(0)u_i$, where $u_i:$ s.t. $P_i^Tu_i = u_i$}
\STATE{Initialise $Z$ s.t. $\exists c, c'\, s.t. \frac{OPT}{B} \leq Z \leq c\frac{OPT}{B} + c'$}
\STATE{Initialise $\theta_1 \in \Theta := \{\theta \, | \, \|\theta\|_1 \leq 1, \, \theta \geq 0\}$}
\STATE{The following procedure launch for each hour separately:}
\FOR{$t := 1\dots T$}
    \STATE{Observe $\{x_{t, i}\}_{j=1}^{m}$ for all $i \in \{0, 1\dots m\}$}:
    \STATE{$[x_{t, i}]_j := \sum_{\xi = 1}^\tau q^T(\Omega_{ij}P^{\xi}_i + (1-\Omega_{ij})P^{\xi}_j)\pi(t)\quad i \in \{1\dots m\}$}
    \STATE{$[x_{t, 0}]_j := \sum_{\xi = 1}^\tau q^TP^{\tau}_j\pi(t)$}
    \STATE{Observe $\bs_t$}
    \IF{$\bs(t) == 0$ or budget is gone}
        \STATE{$a_t := 0$ (no control)}
    \ELSE
        \STATE{Compute optimistic estimations of $\bn$ for each arm $i$:}
        \STATE{$\tn^i := \arg \max_{\tn \in C_t\, s.t. \|\tn\|_1 = n,\, \tn \geq 0}x_{t, i}^T\tn$}
        \STATE{where $C_t$ is from \ref{eq:confidential_ellipsoid}: a confidential ellipsoid for $\bn$}
        \STATE{Compute optimistic estimations of $\Omega_{i,:}$ for each arm $i$:}
        \STATE{$\widetilde{\Omega}_{i,:} := \arg \min_{\xi \in G_t \cap [0, 1]^m}(\xi^T\tn^i)\theta_t$}
        \STATE{where $G_t$ is a confidential ellipsoid for $\Omega_{i, :}$}
        \STATE{$a_t := \arg\max_{i \in [1,\dots, m]} (x_{t, i} - Z\widetilde{\Omega}_{i, :})^T\tn^{i}\theta_t$ } 
    \ENDIF
    \STATE{Play $a_t$}
    \STATE{Observe $s_{t}$ -- the actual ensemble consumption}
    \STATE{Estimate $K(t) := \hat{n}_{t, a_t}^T\widetilde{\Omega}_{a_t, :}$}
    \STATE{Use $s_{t}$ and $K(t)$ to calculate $\hat{n}_{t+1}$ and $\hat{\Omega}_{t+1}$ (see theorem \ref{theorem:ellipsoids}) }
    \STATE{Use $\hat{n}_{t+1}$ and $\hat{\Omega}_{t+1}$ to recalculate $C_{t+1}$ and $G_{t+1}$ (see theorem \ref{theorem:ellipsoids})}
    \STATE{Calculate $\theta_{t+1}$ according to \ref{eq:mwu_theta} with $g_t := \theta_t(K(t) - \frac{B}{T})$}
        \STATE{Calculate next $\pi$ (shared within all procedures):}
        \STATE{$\pi(t+1) = \frac{1}{n}\sum_{j=1}^mn_j(t)(\Omega_{ij}(t)P^{\tau}_i + (1-\Omega_{ij}(t))P^{\tau}_j))\pi(t)$}
\ENDFOR
\end{algorithmic}
\end{algorithm}

To sum up, in \cite{Agrawal2015} the following regret bounds were derived:

\begin{theorem} \textbf{(Agrawal, 2015):} If we find $Z$ from the initialisation of algorithm \ref{algo:linCBwK-m}, then with the probability $\delta$ the algorithm \ref{algo:linCBwK-m} achieves the following regret:
    \[
        \RR_T \leq O\left(\left(\frac{OPT}{B} + 1\right)m\sqrt{T\log \frac{T}{\delta}\log{T}} \right)
    \]

\end{theorem}
