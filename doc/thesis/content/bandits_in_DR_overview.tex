\section{Reinforcement Learning in Demand-Response}
Application of Reinforcement Learning algorithms to Demand-Response problems drew a significant researcher's attention in the recent years. Most of effort is concentrated around two dominant DR architechtures: (1) distributed and (2) centralised with two-sided information channel, both with the goal either to minimise or to stabilise overall energy consumption (sometimes weighted with energy price). For instance, the goal in \cite{Kalathil2015} is to reduce the energy consumption, which is achieved by requesting some subset of subscribers to curtail the consumption. According to the authors, the main issues they are trying to tackle are 1) the mean energy reduction of each client are generally unknown and 2) the frequency of curtailment requests affects the acceptance raito (so called fatugue effects). The architecture here is two-sided and centralized: requested loads response to the aggregator with the real reduction curtailment they will provide. Unfortunately, the whole process is modelled with strict assumptions: only one consumer can be chosen and also the "fatugue" function is supposed to be known. Hence, the whole model looks oversimplified. 

In a similar work of \cite{Ruelens2014} batch Q-learning was adapted for reducing electricity cost of water heaters ensemble. Two-sided centralised architecture was assumed: water heaters send current temperatures of layers, and an aggregator responds to them with "charging priority". Then regarding to this charging priorities the aggregator determines the optimal "priority threshold". Having the priority below the threshold a heater starts heating, and stalls otherwise. A slight reduction in total consumption was demonstrated after 40-45 days of learning, however, no comparison with competitors was provided. 

All the above mentioned models strongly rely on the two-sided information stream, so to be able to communicate with each load individually. However, for majority of residential and commercial energy consumers such architecture require significant infrastructure investments: only new "smart" devices with internet connection may provide the required feedback. Thus, in real-world cases aggregators have to rely on aggregated feedback, and so be unable to distinguish each load's contribution to overall energy consumption. Hence, the only opportunity left is broadcasting the same signal to all devices simultaneously. Let us refer to such case as to "one-sided centralised" architecture.

Little studies is devoted to application of Reinforcement Learning to this type of architecture. The common approach here is to model devices with Markov chains. The paper of \cite{Taylor2014} works with aggregated feedback -- overall energy consumption. The authors apply bayesian inference to reveal individual contribution of each load, which is modelled by 4-states Markov chains. The recent study of \cite{Chertkov2017} adopts MDP approach as well. Online learning techniques was applied to infer the best static control taking into account the mean "natural" behaviour of loads: the aggregator's control should not significantly harm consumers' comfort. 

In this work we develop \cite{Chertkov2017} keeping the architecture requirements the same while making less assumptions on individual behaviour of devices. 