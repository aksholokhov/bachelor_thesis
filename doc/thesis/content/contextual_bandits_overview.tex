
\section{Multiarmed Bandits}
Reinforcement learning can be defined as a learning paradigm concerned with learning to control a complex system so to maximize a numerical performance measure that express some long-term objective \cite{Szepesvari2010}. The most typical setting where reinforcement learning operates is an iterative process of agent-environment interactions (see fig \textit{somefig}). Formally, at the moment $t$ the agent performs an action $a_t$ according to his current policy $\pi_t$ and gets a reward $r_t$ from the environment. The aim of the agent is to maximize the reward $\sum_{t = t_0}^{t_0 + T}r_t$ in a given time horizon $T$, or, equally, to minimize the regret $\bar{R_T} = R^x*_T - \sum_{t = t_0}^{t_0 + T}r_t$ where $R^*_T$ is the best reward the agent can theoretically get with the best policy $\pi^*$ available. However, in most of the real problem we have some additional information associated with each arm or with the situation in general. The vector that represents this information is usually called <<the context>> hence the related setup is called <<contextual multi armed bandit>>. Under the described conditions the agent should take into account the context to achieve better performance. 
 
Multiarmed bandit problem can be defined as a Reinforcement Learning setup with a fixed state of the environment, so the problem here is to learn the best policy. 

The amount of papers devoted to Reinforcement Learning has soared up recent years due to significant advances in \textit{some domains (link)}.

Plenty of papers is devoted to the general multi armed bandit setup. One of the most famous algorithms -- $\varepsilon$-greedy -- was proposed by \cite{Auer2002}: it pulls an arbitrary arm with a probability of $\varepsilon$ and the best arm according to the current policy otherwise. This is due tho the exploration-exploitation balance problem: without this arbitrary steps the agent may fail to gain enough statistics for determining the best policy. However, $\varepsilon$ is a hyperparameter which should be appropriately tuned. Another approach here is so-called <<optimism in the face of uncertainty>> \cite{Lai1985} according to which the learner should choose an arm with the best Upper Confidence Bound. A very successful algorithm which has implemented this technique is UCB1 \cite{Auer2002}, later analysed and improved by \cite{Audibert2009}. The later not only often outperforms UCB1, but also had shown to be essentially unimprovable under the assumption that the variance of the reward associated with some of the actions are small.

In contrast to the regular bandit problem, the contextual bandit setup tends to be tougher problem to solve. One approach is to assume the particular dependency model between the arms' context $x_{a,t}$ and the expected reward $r_t$. One possible model is a linear one: \begin{align}
        &\label{eq:reward_linear_assumption}
        r_{a,t} = \bar{r_{a,t}} + \varepsilon_{a,t} \\
        s.t.\, & \bar{r_{a,t}}  = \E[r_{t,a}|x_{t,a}] = x_{t,a}^T\theta^* \\
        & \E \varepsilon_{a,t}  = 0 
    \end{align}
    
    For this linear case \cite{Li2010} proposed an adaptation of UCB1 approach: LinUCB. Later \cite{Abbasi-Yadkori2011} provided better theoretical analysis by eliminating the assumption that the reward is identically distributed over time and arms, which is mostly far from true. The similar case with different regularization strategy was examined by \cite{Auer2003}. Another heuristics for balancing exploration and exploitation is known as Thompson Sapling was adapted to the linear reward model by \cite{Agrawal2013}. 
