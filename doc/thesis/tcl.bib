@article{Mahadevan2012SparseDescent,
    title = {{Sparse Q-learning with Mirror Descent}},
    year = {2012},
    journal = {arXiv.org},
    author = {Mahadevan, Sridhar and Liu, Bo},
    volume = {cs.LG},
    url = {http://arxiv.org/abs/1210.4893},
    isbn = {9780974903989},
    arxivId = {abs/1210.4893}
}

@article{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective. What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming. We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:Users/aksholokhov/Documents/Papers/Szepesv{\'{a}}ri/2010/Synthesis Lectures on Artificial Intelligence and Machine Learning/Szepesv{\'{a}}ri - 2010 - Algorithms for Reinforcement Learning.pdf:pdf},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
month = {jan},
number = {1},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
volume = {4},
year = {2010}
}


@article{Lai1985,
abstract = {The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log {\textless}e1{\textgreater}n{\textless}/e1{\textgreater}). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated},
author = {Lai, T. L. and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
file = {:Users/aksholokhov/Documents/Papers/Lai, Robbins/1985/Advances in Applied Mathematics/Lai, Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf:pdf},
isbn = {0196-8858},
issn = {10902074},
journal = {Advances in Applied Mathematics},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
number = {1},
pages = {4--22},
title = {{Asymptotically efficient adaptive allocation rules}},
volume = {6},
year = {1985}
}

@article{Audibert2009,
abstract = {Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Audibert, Jean Yves and Munos, R{\'{e}}mi and Szepesv{\'{a}}ri, Csaba},
doi = {10.1016/j.tcs.2009.01.016},
file = {:Users/aksholokhov/Documents/Papers/Audibert, Munos, Szepesv{\'{a}}ri/2009/Theoretical Computer Science/Audibert, Munos, Szepesv{\'{a}}ri - 2009 - Exploration-exploitation tradeoff using variance estimates in multi-armed bandits.pdf:pdf},
isbn = {0304-3975},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Bernstein's inequality,Exploration-exploitation tradeoff,High-probability bound,Multi-armed bandits,Risk analysis},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
number = {19},
pages = {1876--1902},
title = {{Exploration-exploitation tradeoff using variance estimates in multi-armed bandits}},
volume = {410},
year = {2009}
}

@inproceedings{Li2010,
abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5{\%} click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1003.0146},
author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
booktitle = {Proceedings of the 19th international conference on World wide web - WWW '10},
doi = {10.1145/1772690.1772758},
eprint = {1003.0146},
file = {:Users/aksholokhov/Documents/Papers/Li et al/2010/Proceedings of the 19th international conference on World wide web - WWW '10/Li et al. - 2010 - A contextual-bandit approach to personalized news article recommendation.pdf:pdf},
isbn = {9781605587998},
issn = {9781605587998},
keywords = {contextual bandit,exploitation dilemma,exploration,personalization,recommender sys-,tems,web service},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
pages = {661},
publisher = {ACM Press},
title = {{A contextual-bandit approach to personalized news article recommendation}},
url = {http://arxiv.org/abs/1003.0146{\%}0Ahttp://dx.doi.org/10.1145/1772690.1772758 http://portal.acm.org/citation.cfm?doid=1772690.1772758},
year = {2010}
}

@article{Abbasi-Yadkori2011,
abstract = {OFUL: almost LinUCB, but with a direct proof},
author = {Abbasi-Yadkori, Yasin and Pal, David and Szepesv{\'{a}}ri, Csaba},
file = {:Users/aksholokhov/Documents/Papers/Abbasi-yadkori/2010/Unknown/Abbasi-yadkori - 2010 - Improved Algorithms for Linear Stochastic Bandits.pdf:pdf},
isbn = {9781618395993},
journal = {Neural Information Processing Systems},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
pages = {1--19},
title = {{Improved Algorithms for Linear Stochastic Bandits}},
year = {2011}
}

@article{Agrawal2013,
abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of {\$}\backslashtilde{\{}O{\}}(d{\^{}}{\{}3/2{\}}\backslashsqrt{\{}T{\}}){\$} (or {\$}\backslashtilde{\{}O{\}}(d\backslashsqrt{\{}T \backslashlog(N){\}}){\$}), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of {\$}\backslashsqrt{\{}d{\}}{\$} (or {\$}\backslashsqrt{\{}\backslashlog(N){\}}{\$}) of the information-theoretic lower bound for this problem.},
archivePrefix = {arXiv},
arxivId = {1209.3352},
author = {Agrawal, Shipra and Goyal, Navin},
eprint = {1209.3352},
file = {:Users/aksholokhov/Documents/Papers/Agrawal, Goyal/2013/Proceedings of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA/Agrawal, Goyal - 2013 - Thompson Sampling for Contextual Bandits with Linear Payoffs.pdf:pdf},
issn = {1938-7228},
journal = {Proceedings of the 30th International Conference on Machine Learning, Atlanta, Georgia, USA},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
month = {sep},
title = {{Thompson Sampling for Contextual Bandits with Linear Payoffs}},
url = {http://arxiv.org/abs/1209.3352},
volume = {28},
year = {2013}
}

@article{Auer2003,
abstract = {We show how a standard tool from statistics namely confidence bounds can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
author = {Auer, Peter},
doi = {10.1162/153244303321897663},
file = {:Users/aksholokhov/Documents/Papers/Auer/2003/Journal of Machine Learning Research/Auer - 2003 - Using Confidence Bounds for Exploitation-Exploration Trade-offs.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {bandit problem,exploitation exploration,learning,linear value function,online learning,reinforcement},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
number = {3},
pages = {397--422},
title = {{Using Confidence Bounds for Exploitation-Exploration Trade-offs}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {3},
year = {2003}
}



@article{Na2011OptimalNetworks,
    title = {{Optimal demand response based on utility maximization in power networks}},
    year = {2011},
    journal = {Power and Energy Society General Meeting, 2011 IEEE},
    author = {Na, Li and Lijun, Chen and Low, Steven H and Li, Na and Chen, Lijun},
    pages = {1--8},
    url = {http://dx.doi.org/10.1109/PES.2011.6039082},
    isbn = {19449925},
    doi = {10.1109/PES.2011.6039082}
}

@article{Zhou2015ABandits,
    title = {{A Survey on Contextual Multi-armed Bandits}},
    year = {2015},
    author = {Zhou, Li},
    url = {http://arxiv.org/abs/1508.03326},
    arxivId = {1508.03326}
}

@book{Ghavamzadeh2015BayesianSurvey,
    title = {{Bayesian Reinforcement Learning: A Survey}},
    year = {2015},
    booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
    author = {Ghavamzadeh, Mohammed and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
    number = {5-6},
    pages = {359--483},
    volume = {8},
    url = {http://www.nowpublishers.com/article/Details/MAL-049},
    isbn = {2200000049},
    doi = {10.1561/2200000049},
    issn = {1935-8237},
    pmid = {18255791},
    arxivId = {1405.4980}
}

@article{Kuleshov2014AlgorithmsProblems,
    title = {{Algorithms for multi-armed bandit problems}},
    year = {2014},
    author = {Kuleshov, Volodymyr and Precup, Doina},
    pages = {1--32},
    volume = {1},
    url = {http://arxiv.org/abs/1402.6028},
    isbn = {0-89871-605-5},
    doi = {10.1145/1109557.1109659},
    arxivId = {1402.6028}
}

@article{Cabras2007ExtremeFramework,
    title = {{Extreme Value Analysis within a Parametric Outlier Detection Framework}},
    year = {2007},
    journal = {Applied Stochastic Models in Business and Industry},
    author = {Cabras, S and Morales, J},
    number = {January},
    pages = {157--164},
    volume = {23},
    isbn = {15241904},
    doi = {10.1002/asmb},
    issn = {1524-1904},
    pmid = {35395390},
    keywords = {generalized pareto distribution, partial posterior predictive distribution, threshold selection}
}

@article{Vardakas2015AAlgorithms,
    title = {{A Survey on Demand Response Programs in Smart Grids: Pricing Methods and Optimization Algorithms}},
    year = {2015},
    journal = {IEEE Communications Surveys and Tutorials},
    author = {Vardakas, John S. and Zorba, Nizar and Verikoukis, Christos V.},
    number = {1},
    pages = {152--178},
    volume = {17},
    isbn = {1553-877X},
    doi = {10.1109/COMST.2014.2341586},
    issn = {1553877X},
    arxivId = {arXiv:1502.03908v1},
    keywords = {Smart grid, demand response, optimization algorithms, pricing methods}
}

@article{HalderOptimalLoads,
    title = {{Optimal Power Consumption for Demand Response of Thermostatically Controlled Loads}},
    author = {Halder, Abhishek and Geng, Xinbo and Fontes, Fernando A C C and Kumar, P R and Xie, Le},
    url = {https://arxiv.org/pdf/1609.07229.pdf},
    arxivId = {1609.07229}
}

@article{Lakshmanan2016ImpactExperiment,
    title = {{Impact of thermostatically controlled loads' demand response activation on aggregated power: A field experiment}},
    year = {2016},
    journal = {Energy},
    author = {Lakshmanan, Venkatachalam and Marinelli, Mattia and Kosek, Anna M. and N{\o}rg{\aa}rd, Per B. and Bindner, Henrik W.},
    pages = {705--714},
    volume = {94},
    publisher = {Elsevier Ltd},
    url = {http://dx.doi.org/10.1016/j.energy.2015.11.050},
    doi = {10.1016/j.energy.2015.11.050},
    issn = {03605442},
    keywords = {Aggregator, Demand response, Domestic energy resources, Flexible electricity demands, Load management, Smart grid}
}

@article{Trovato2015DemandValue,
    title = {{Demand Response from Thermostatically Controlled Loads : Modelling , Control and System-Level Value}},
    year = {2015},
    journal = {Thesis{\_}Imperial college london},
    author = {Trovato, Vincenzo},
    number = {February}
}

@article{Auer2002,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we showthat the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.6028v1},
author = {Auer, Peter and Cesa-Bianchi, Nicol{\`{o}} and Fischer, Paul},
doi = {10.1023/A:1013689704352},
eprint = {arXiv:1402.6028v1},
file = {:Users/aksholokhov/Documents/Papers/Auer, Cesa-Bianchi, Fischer/2002/Machine Learning/Auer, Cesa-Bianchi, Fischer - 2002 - Finite-time Analysis of the Multiarmed Bandit Problem.pdf:pdf},
isbn = {089871611X | 9780898716115},
issn = {08856125},
journal = {Machine Learning},
keywords = {Campaign management support systems,Data handling,Data mining algorithms,Density based clustering,Evolving data streams,Multivariate anal,Predictive analytics,Predictive process,adaptive allocation rules,adaptive monte carlo,all or part of,back,bagged,bandit problems,bayesian inference,bayesian networks,c{\_}and{\_}e,cation,classi,click feed-,click-through-rate,collaborative learning,content-based learning,digital advertising,dual averaging,event discovery,event retrieval,exploitation and exploration,finite horizon regret,generalized pareto distribution,hamiltonian monte carlo,linear models,linear regression,locally weighted regression,markov chain monte carlo,markov models,model trees,models,monte carlo,multi-touch attribution model,naive bayes,neural networks,online advertising,or hard copies of,org entities,partial posterior predictive distribution,permission to make digital,regression,rule induction,schema,student{\_}modeling,tfidf-based,this work for,threshold selection,web events},
mendeley-groups = {Mathematics/Machine learning/Reenforcement learning},
number = {2/3},
pages = {235--256},
pmid = {18292226},
title = {{Finite-time Analysis of the Multiarmed Bandit Problem}},
url = {http://link.springer.com/article/10.1023/A:1020231107662{\%}5Cnhttp://link.springer.com/chapter/10.1007/3-540-70659-3{\_}2{\%}5Cnhttp://www.datashaping.com/ABbook5.pdf{\%}5Cnhttp://phyusdb.files.wordpress.com/2013/03/monte-carlo-methods-second-revised-and-enlarged-edition.pdf http://link.springer.com/10.1023/A:1013689704352},
volume = {47},
year = {2002}
}
